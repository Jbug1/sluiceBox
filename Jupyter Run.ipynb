{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e825025d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T03:45:02.675613Z",
     "start_time": "2022-07-15T03:45:02.155802Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pybloom_live as bloom\n",
    "import time\n",
    "from Bio import SeqIO as seq\n",
    "import sys\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c4595a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T03:46:29.320653Z",
     "start_time": "2022-07-15T03:46:29.280680Z"
    }
   },
   "outputs": [],
   "source": [
    "def choose_genome_sparsity(genome_filesize, read_filesize, read_length, key_size, num_files, flex_factor=2, average_exon_size = 100):\n",
    "    \"\"\"\n",
    "    function to choose genome indices and read indices to optimize for runtime\n",
    "    genome and read filesizes only need to be in the same units\n",
    "\n",
    "    read_length: length of RNAseq reads in fasta file\n",
    "    key_size: int: size of partial keys to use\n",
    "    num_files: number of RNAseq reads\n",
    "    flex_factor: how many discrepancies per read we can accomodate\n",
    "    average_exon_size: self explanatory\n",
    "\n",
    "    returns: factor by which we must reduce number of keys in the genome\n",
    "    \"\"\"\n",
    "    \n",
    "    if key_size>=average_exon_size or key_size >= read_length:\n",
    "        return(\"err: key size is too large\")\n",
    "    \n",
    "    num = genome_filesize*(1-(key_size/average_exon_size))\n",
    "    denom = (read_filesize/read_length)*flex_factor*num_files\n",
    "    \n",
    "    out = int((num/denom)**0.5)\n",
    "    \n",
    "    if out==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return out\n",
    "\n",
    "def estimate_perc_full(key_size, bases,exons):\n",
    "    \"\"\"\n",
    "    size: total number of bases\n",
    "    key_length: length of partial key\n",
    "    exons: total number of exons in the genome\n",
    "    sparsity_factor: 1/sparsity_factor=the proportion of bases we use as starting index when creating filter\n",
    "\n",
    "    returns: estimate of how full a set filter is\n",
    "    *****have dropped sparsity factor for now___need to think this thru******\n",
    "    \"\"\"\n",
    "    \n",
    "    samps = ((bases/exons)-key_size)*exons\n",
    "    expected_unique=(1-(1-1/samps)**(4**key_size))*samps\n",
    "    return expected_unique/4**key_size\n",
    "\n",
    "def generate_key_size(bases, exons, fpr=0.01):\n",
    "    \"\"\"\n",
    "    picks the smallest key size that will still give us less than fpr% expected false positives in set filter\n",
    "    bases: total number of bases in genome used to make filter\n",
    "    exons: total number of exons in genome used to make filter\n",
    "    fpr: max acceptable false positive rate\n",
    "    sparsity_factor: 1/sparsity_factor=the proportion of bases we use as starting index when creating filter\n",
    "\n",
    "    returns: minimum key size that meets fpr goals\n",
    "    \"\"\"\n",
    "\n",
    "    #start with intial size\n",
    "    key_size = 10\n",
    "    \n",
    "\n",
    "    #increase size until percentage full estimate falls below 1%\n",
    "    while estimate_perc_full(key_size, bases, exons) >fpr:\n",
    "        \n",
    "        key_size+=1\n",
    "\n",
    "    return key_size\n",
    "\n",
    "def estimate_bits_needed (key_size, ave_exon_size, num_exons, sparsity):\n",
    "    \"\"\"\n",
    "    estmiate bits needed for bloom filter based on size of keys and how many of them we will have\n",
    "\n",
    "    key_len: length of partial keys we are using\n",
    "    ave_exon_size: average size of exon in GENOME...what we use to populate filter\n",
    "    num_exons: number of exons in the genome\n",
    "    sparsity: sparsity factor for choosing keys from genome exons\n",
    "\n",
    "    returns: number of bits to set filter with\n",
    "    \"\"\"\n",
    "    total_keys = (ave_exon_size - key_size)*num_exons/sparsity\n",
    "    return sys.getsizeof(\"a\"*key_size)*total_keys\n",
    "\n",
    "class filter_object():\n",
    "    \"\"\"\n",
    "    object to house the filter type and its attributes\n",
    "    genome: filepath to genome from which we will build filter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self, genome, key_size, sparsity, bloombits, set_filter = False, fpr=0.01):\n",
    "        self.genome = genome\n",
    "        self.key_size = key_size\n",
    "        self.sparsity = sparsity\n",
    "        self.set_filter = set_filter\n",
    "        self.bloombits = bloombits\n",
    "        self.fpr=fpr\n",
    "    \n",
    "    def generate_filter(self, verbose = False):\n",
    "        \"\"\"\n",
    "        set_filter: implement filter using has set rather than array\n",
    "        indices: generate the filter with partial keys beginning at these indices\n",
    "        filter object: filter array to be populated with seen reads, can experient with augmenting existing filters later\n",
    "        could have if/then stacked filter where we just keep grabbing larger partial keys (might be more efficient way\n",
    "        to access reads and score them)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #check to see if there are indices passed to filter obj\n",
    "        #assumes that all exons are the same size\n",
    "        #if not, create range object that is the length of the reads in genome fasta file\n",
    "        # if self.indices==False:\n",
    "        #     with open(self.genome) as input:\n",
    "        #         self.indices = range(len(list(seq.parse(input,\"fasta\").seq[0])))\n",
    "           \n",
    "            \n",
    "        #use the set filter rather than array\n",
    "        if self.set_filter:\n",
    "            filter_ = set()\n",
    "\n",
    "        #if using bloom    \n",
    "        else:\n",
    "            filter_ = bloom.BloomFilter(capacity = self.bloombits, error_rate = self.fpr)\n",
    "            \n",
    "        #open genome file and iterate over reads       \n",
    "        with open(self.genome) as input:\n",
    "            for record in seq.parse(input,\"fasta\"):\n",
    "\n",
    "                #initialize variable to tell us the max position we can start from\n",
    "                max_index=len(record)-self.key_size-1\n",
    "\n",
    "                #initialize helper var to iterate over key\n",
    "                j=0\n",
    "\n",
    "                while j<max_index:\n",
    "\n",
    "                    #add key to filter and increment j \n",
    "                    filter_.add(record.seq[j:j+self.key_size])\n",
    "                    j+=self.sparsity\n",
    "        \n",
    "\n",
    "        self.filter_ = filter_\n",
    "        \n",
    "\n",
    "    def fast_check(self, in_path, out_path, check_indices):\n",
    "        \"\"\"\n",
    "        in_path: filepath to RNAseq input fasta file\n",
    "        out_path_filepath to write new fasta file to\n",
    "        check_indices: indices of the RNAseq file to check in filter\n",
    "        key_size: num bases in partial key\n",
    "        \"\"\"  \n",
    "\n",
    "        with open(out_path, \"w\") as kept_reads:\n",
    " \n",
    "            #open RNAseq file and create iterator to pass over reads\n",
    "            with open(in_path) as reads:\n",
    "                for read in seq.parse(reads,\"fastq\"):\n",
    "                    \n",
    "                    #iterate over indices to check filter for\n",
    "                    for check_index in check_indices:\n",
    "\n",
    "                        #check to see if partial key in filter\n",
    "                        if read.seq[check_index:check_index+self.key_size] in self.filter_:\n",
    "                            \n",
    "                            #add read to output fasta file and move on to the next read\n",
    "                            seq.write(read, kept_reads, \"fastq\")\n",
    "                            break\n",
    "\n",
    "    def sample_indices (self, in_path, out_path):\n",
    "        \"\"\"\n",
    "        sample possible check indices (of the minimum size we need) and determine set of indices with the lowest hit rate\n",
    "        should be able to guarantee that all prpper reads align while minimizing overall hits\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "        \n",
    "def main(genome, reads_path, set_filter_, fpr=0.05, premade_filter = False):\n",
    "    \"\"\"\n",
    "    main function defines the parameters of the filter, loads previously built filter or generates a new one given genome input reference\n",
    "    inputs are filter object paramters plus option to point to premade filter in memory\n",
    "    read_path_folder: folder to load all reads from \n",
    "    output_paths: list: same length as read paths, output file locations to write new fasta files to\n",
    "    \"\"\"\n",
    "    timey=time.time()\n",
    "    #instantiate list of read filenames\n",
    "    reads_list = os.listdir(reads_path)\n",
    "\n",
    "    #process set filter variable from system input\n",
    "    if set_filter_ in {\"True\",\"true\",\"T\",\"t\"}:\n",
    "        set_filter = True\n",
    "    else:\n",
    "        set_filter=False\n",
    "\n",
    "    #grab respective filesizes...assumes the first file size in reads is representative\n",
    "    genome_filesize = float(os.path.getsize(genome))\n",
    "    \n",
    "    print(f\"examined_genome_size {genome_filesize} in {time.time()-timey}\")\n",
    "    timey=time.time()\n",
    "    \n",
    "    read_filesize = 0\n",
    "    num_files=0\n",
    "    for file in os.listdir(reads_path):\n",
    "\n",
    "        full_path = os.path.join(reads_path,file)\n",
    "        print(full_path)\n",
    "        read_filesize += float(os.path.getsize(full_path))\n",
    "\n",
    "        num_files+=1\n",
    "\n",
    "    print(f\"gathered total read size {read_filesize} in {time.time()-timey}\")\n",
    "    timey=time.time()\n",
    "\n",
    "    #decide on key size...will choose smallest with less than 1% false positives in set filter\n",
    "    #first grab total bases\n",
    "    bases=0\n",
    "    exons=0\n",
    "    with open(genome) as genome_:\n",
    "        for exon in seq.parse(genome_,\"fasta\"):\n",
    "            bases+=len(exon.seq)\n",
    "            exons+=1\n",
    "    ave_exon_size = int(bases/exons)\n",
    "\n",
    "    bases_read = 0\n",
    "    exons_read=0\n",
    "    \n",
    "    first_1000_read_lens = np.zeros(1000)\n",
    "    x=0\n",
    "    with open(os.path.join(reads_path, os.listdir(reads_path)[0])) as read1:\n",
    "        for exon in seq.parse(read1,\"fastq\"):\n",
    "            if x<1000:\n",
    "                first_1000_read_lens[x]=len(exon.seq)\n",
    "                x+=1\n",
    "\n",
    "            bases_read+=len(exon.seq)\n",
    "            exons_read+=1\n",
    "\n",
    "    print(f\"calcd average genome and read exon len in {time.time()-timey}\")\n",
    "    timey=time.time()\n",
    "\n",
    "    first_1000_read_lens.sort()\n",
    "    print(f\"5th and 25th percentile read lengths: {first_1000_read_lens[50]} {first_1000_read_lens[250]}\")\n",
    "\n",
    "    #determine average read length for first read file\n",
    "    ave_read_length = int(bases_read/exons_read)\n",
    "\n",
    "    print(f\"average read length: {ave_read_length}\")\n",
    "\n",
    "    #generate_key_size\n",
    "    key_size = generate_key_size(bases=bases, exons=exons)\n",
    "\n",
    "    print(f\"generated key size {key_size} in {time.time()-timey}\")\n",
    "    timey=time.time()\n",
    "    \n",
    "    #set flex factor to 2 for now\n",
    "    flex_factor = 6\n",
    "\n",
    "    #decide on ideal mix or genome and transcriptome incdices\n",
    "    sparsity = choose_genome_sparsity(genome_filesize, read_filesize, ave_read_length, key_size, num_files, flex_factor, ave_exon_size)\n",
    "\n",
    "    print(f\"chose sparsity: {sparsity} in {time.time()-timey}\")\n",
    "    timey=time.time()\n",
    "\n",
    "    #determine number of bits for bloom filter\n",
    "    bloom_bits = estimate_bits_needed(key_size = key_size, ave_exon_size=ave_exon_size, num_exons=exons, sparsity=sparsity)\n",
    "\n",
    "    #create filter object\n",
    "    filter_obj = filter_object(genome=genome, key_size=key_size, sparsity=sparsity ,set_filter = set_filter, bloombits=bloom_bits)\n",
    "\n",
    "    #check for premade filter and generate new one if not present\n",
    "    if premade_filter:\n",
    "        filter_obj.filter_ = premade_filter\n",
    "\n",
    "    else:\n",
    "        filter_obj.generate_filter()\n",
    "\n",
    "    print(\"generated filter \", time.time()-timey)\n",
    "    timey=time.time()\n",
    "\n",
    "    #generate indices to check...populate empty array with indices based on flex factor and filter sparsity\n",
    "    check_indices = np.zeros(flex_factor*sparsity+sparsity).astype(int)\n",
    "\n",
    "\n",
    "    change_ind=0\n",
    "    check_ind=-1\n",
    "    for i in range(flex_factor+1):\n",
    "    \n",
    "        if i>0:\n",
    "            check_ind+=key_size-1\n",
    "        for j in range(sparsity):\n",
    "        \n",
    "            check_ind+=1\n",
    "            check_indices[change_ind] += check_ind\n",
    "            change_ind+=1\n",
    "\n",
    "    print(check_indices)\n",
    "    #iterate over filepaths for RNAseq fastas and write the reads we want to keep to the corresponding output path\n",
    "    for i in range(len(reads_list)):\n",
    "\n",
    "        filter_obj.fast_check(os.path.join(reads_path, reads_list[i]), os.path.join(reads_path,\"out\"+reads_list[i]), check_indices)\n",
    "\n",
    "    print(\"checked reads \", time.time()-timey)\n",
    "    timey=time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5f083d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T03:52:58.910003Z",
     "start_time": "2022-07-15T03:49:07.351822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examined_genome_size 5218997.0 in 0.0\n",
      "C:\\Users\\jonah\\School\\ASI\\bloom\\syntheticData\\ecoli\\reads\\trimmed\\SRR19897826_trim.fastq\n",
      "gathered total read size 1152465882.0 in 0.0\n",
      "calcd average genome and read exon len in 45.572789669036865\n",
      "5th and 25th percentile read lengths: 106.0 228.0\n",
      "average read length: 218\n",
      "generated key size 15 in 0.0\n",
      "chose sparsity: 1 in 0.0\n",
      "generated filter  36.22866344451904\n",
      "[ 0 15 30 45 60 75 90]\n",
      "checked reads  149.70497798919678\n"
     ]
    }
   ],
   "source": [
    "main(\"C:\\\\Users\\\\jonah\\\\School\\\\ASI\\\\bloom\\\\syntheticData\\\\ecoli\\\\genome\\\\GCF_000157115.2\\\\unplaced.scaf.fna\", \"C:\\\\Users\\\\jonah\\\\School\\\\ASI\\\\bloom\\\\syntheticData\\\\ecoli\\\\reads\\\\trimmed\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d814a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T22:20:36.718068Z",
     "start_time": "2022-07-18T22:20:36.700092Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (4043207311.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pd.read_txt(\"C:\\Users\\jonah\\School\\ASI\\bloom\\syntheticData\\ecoli\\reads\\outs2\\filter.py1\\res.sam\")\u001b[0m\n\u001b[1;37m                                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "pd.read_txt(\"C:\\Users\\jonah\\School\\ASI\\bloom\\syntheticData\\ecoli\\reads\\outs2\\filter.py1\\res.sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "853c9117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T01:55:54.554560Z",
     "start_time": "2022-07-19T01:55:54.045465Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "    \n",
    "bigs = np.random.randint(10000000,11000000,int(1e8))\n",
    "smalls=np.random.randint(0,1e6,int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9510e053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T01:48:56.069158Z",
     "start_time": "2022-07-19T01:48:53.406124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631772 631919\n"
     ]
    }
   ],
   "source": [
    "big_set=set()\n",
    "small_set=set()\n",
    "\n",
    "big_collisions=0\n",
    "small_collisions=0\n",
    "for item in bigs:\n",
    "    if hash(str(item)) in big_set:\n",
    "        big_collisions+=1\n",
    "        \n",
    "    else:\n",
    "        big_set.add(hash(str(item)))\n",
    "        \n",
    "for item in smalls:\n",
    "    if hash(str(item)) in small_set:\n",
    "        small_collisions+=1\n",
    "        \n",
    "    else:\n",
    "        small_set.add(hash(str(item)))\n",
    "        \n",
    "print(len(small_set), len(big_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82cf7a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T16:57:54.885469Z",
     "start_time": "2022-07-23T16:57:54.869498Z"
    }
   },
   "outputs": [],
   "source": [
    "class read():\n",
    "    \n",
    "    def __init__(self, name, illumin, length, seq, quality):\n",
    "        self.name=name\n",
    "        self.illumin=illumin\n",
    "        self.length=length\n",
    "        self.seq=seq\n",
    "        self.quality=quality\n",
    "        \n",
    "\n",
    "def open_file(filepath):\n",
    "    \n",
    "    with open(\"C:\\\\Users\\\\jonah\\\\School\\\\ASI\\\\bloom\\\\sample.fastq\") as sample:\n",
    "        \n",
    "        while True: \n",
    "            \n",
    "            line1,line2,line3,line4 = tuple(next(sample) for i in range(4))\n",
    "               \n",
    "            name, illumin, length = line1.split()\n",
    "            seq=line2\n",
    "            quality=line4\n",
    "            \n",
    "            newread = read(name, illumin, length, seq,quality)    \n",
    "            print(newread.seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ac06f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T16:57:55.572424Z",
     "start_time": "2022-07-23T16:57:55.522426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAGTGTCTCCAACCATCAGCACGCGTGAACGGTCTG\n",
      "\n",
      "GTTCGGCATCAACAAAACGGCGTTTGGCGGCAAACCAACGTGGGCTGTCTGGCAGGAAGAAGACACCAATCAGCAGCAAAATTGCCGGGATGATAATCACACCCAGCATCCAGCGCCATGCACCGGTGTAGCTGAAG\n",
      "\n",
      "GATAAAAAGGATGCTCAGGAGGCAACGACGTCGAAACAATAATGGCATCGACCTGACGCTGTAAAAGGTGCTCAATGCAGCGCATTTCGTTGTCTGGCTGATCTTCTGAGCAGGCAATCAGCAGTTGATAACCCCGTTGCCGCGCCTGGCGTTCAAGATAGTTAGCGATGCGGGTATAGCTGGTGTTCTCCAGATCGGGGATCACAAGACCAATAGAACGTGTGCGTCCAGCACGA\n",
      "\n",
      "GTATCTGGACGCGCTGGAGCAAGAAATCGTCCATCGTGCACCGCTGTTCGCCGGGCGTCACGTCAGCCAGCTGCACTGGGGCGGCGGAACGCCGACGTATCTTAATAAAGCGCAAATCAGCCGCCTGATGAAGCTGCTGCGCGAAAACTTCCAGTTCAATGCCGATGCGGAGATTTCGATCGAAGTCGATCCGCGGGAAATCGAACTGGATGTACTCGATCATTTACGCGCCGAGG\n",
      "\n",
      "AATGGCATAAATACCGACAAAACCAATGCCGACTGTTAATGACGAACGTAAGGCTTTAGCGGGTTTTACTTTAAAACAGAGAGCAAGAATAAATAACACCAACGGCAACATTACCGTCGGTCCAAATCCCAATATATATTGCACATTATTATAGAG\n",
      "\n",
      "CATGGTCTCTGGTGGCACCCCCTCGCGCAATCTGGCCTTCCAGGCACTGAAAGCGGATTTCTTCGGGTTTGCCCCAGCACGCAACGGGTATTCCCGCCAGACCTGTTCGAACACATCCGGATAATCCACTCGTCCCACAGACTGCCCGGTGCTTTCCGGGACTACCCGATCGGCTTCCCGCTGAATGGCGGAATCGGCTTCAGGCTGCTGCAGTTGGTGTGATTGCTCCGACCCT\n",
      "\n",
      "TCGACGTCCCAGCAATCCTTGCACTTTTAATGAATAGCGATTCTGCTCCTCGCGCTACGCTTCGTCTTATGCATCATGCATCCCCAAACGACTCGCCAGAACGTCAAGATACTTTTCCTGCACCTGTTGCTGGCACTTAATATCACTCATC\n",
      "\n",
      "GGAGCTTACGTGGTTTCCCGGCTGGTGACCAGAGAAATGGGGATGAGGCTTCGCCCCTTCAAGGGGGAAAAAGAAAAAAAATTCTGATCAGCACAAAATCGGGTGAAAACCCTGATTCACCTCACATTCTGTCTCTTATACACATCTCCGAGCCCACGAGACAAAGGCGAATCTCGTATGCCGTCTTCTGCTTGAAAAAAAACACATC\n",
      "\n",
      "GCTGATCACTAAGCAGCTCACTGTTACATCATGCTGGACGCGATAATTGGAATCTCACGCAGGGAGTAATGTGTAAATAGCAAAATCGCCTGATGCGCTACGCCTGTCAGTCCTTCAAATAACATACACTTTATTGAATTTTCATGACTTTGTAGATCGGATAAGACGCGTTAGC\n",
      "\n",
      "AGCTTAAGAATTACGCCGCTAGTGTTCCTGGCTATTTTGTCTTGTTCACTCC\n",
      "\n",
      "CCGTCTGATCCCGCTCAGTCTCGCCTGGCGTTCGCTTGCCGATCATCAAAGTGCCGCCGCTAACGACATTCGTCAGATGATTCAGGATAACCGTCTGCGGCTGATGCAACTGGCTGGGCCCGGAGCGCTCTTTACCTGGTGGGGTGAAGATGGCAATGGTGACGCCTTCCTTACGG\n",
      "\n",
      "ATCCGATTATTGTCGCCCAGGATGGTTCGCTTTCTGGTCCTGGTATGGCTTGTACCACAGTCGCCAAACAGACTTACGCCCTGCCTGCACCTCCCGATTTAAGCGGTGGCGCGGGAACAAGTTCAGTGTCTGGCCCGCAGGGTGACATTCTTCCGGTCAGTAATTCGACGCTAAAAAGCGAAGATCCGACCGGCGCGCCGGTAACCAGCAGCGGTTTCCTCGGCGCACCAACGACC\n",
      "\n",
      "AAAAAGAGATTATTGTTCCTGATATGGAAACGAAAATCGCCGCCCATCTGGCGGGCGTTGGCATTGGTTTTTTGCCAAAATCGCTTTGCCAGTCAATGATCGATAATCAACAACTGGTCAGCCGGGTAATCCCAACGATGCGCCCTCCTTCGCCATTGAGTCTGGCATGGCGCAAATTTGGCAGCGGCAAAGCGGTAGAAGATATTGTGACCTTGTTTACCCAGCGCAGGCCGGAA\n",
      "\n",
      "CAGCATGTTGAATATTTCGTGCACGGGTGACTCTGAATAAGGATTAAACCTCTATGATAGGTAACCTGAAGGCTGATGACCAGCAGGCCGTTTTTGAGGAAAAACCACAATTTTAAAAGGGGATTGATAGATAAATGGTGTATTAGCAATGCATCTCTTGATAATGCACTTCTGGTTAGTAAATTTATTGAAATTATTATGTTAATAAAATGAGTGGTAATTTTCTTGTTAGCGC\n",
      "\n",
      "CCCGCTGCCGACACGCAAAGAGCGCTTCACTGTTCTGATCTCCCCGCACGTCAACAAAGACGCGCGCGATCAGTACGAAATCCGTACTCACTTGCGTCTGGTTGACATCGTTGAGCCAACCGAGAAAACCGTTGATGCTCTGATGCGTCTGGATCTGGCTGCCGGTGTAGACGTGCAGATCAGCCTGGGTCTGTCTCTTATACACATCTGACGCTGCCGACGATATGCAGTGTG\n",
      "\n",
      "ATGCCATTAATGGCCTATTATAACGTCCCTGCTATGTGTCGGTCGTCGCTGGCCATGTATAACAC\n",
      "\n",
      "TGAATTGCTGTTTATTCGCCATGAAGGCAATGAGCTGGAGCTGGAAATTCACTGCTCAAAAGGCACTTATATCCGCACCATCATTGATGACCTGGGTGAAAAACTCGGCTGTGGCGCGCATGTGATTTACCTGCGTCGTCTGGCGGTAAGTAAATATCCGGTTGAACGGATGGTGACCCTGGAGCACCTGCGTGAACTGGTCGAACAAGCCGAACAGCAGGATATTCCGGCTGCGG\n",
      "\n",
      "CGTATCCGCTCCCCAGCGCGTGGACCATACCAGCTTTTCCACTTCTTCTTCGATGGAAGAGGTGACCGCCGAGTTGCCGATATTGGCGTTAACTTTTACCAGGAAATTGCGACCAATAATCATCGGCTCCGATTCCGGATGATTAATGTTGGCCGGGATAATCGCACGTCCGGCAGCAACTTCATCACGGACAAATTCCGCAGTGATATTTTCCGGCAGACGTGCGCCAAAGCTC\n",
      "\n",
      "GTATACCAAAAGCAAAACGTTTAAAACTGAAGCTGGCGCGCGTCGCTGGTTAGCCAGAAACACTGACTGATGAGGTTGACGATGGAATTTAAAGATTTACCTCCTTCAATCCAGGAGATTGCAGCACACACACTTCGTCATCGTCTGAACGAACTTGAATTGGAATCGGTAACGAAAAAAGACACTGATAATATGGCTCGTAATGTGCGCGTTGCGTTTACCGGATTGGATTT\n",
      "\n",
      "GGGCGATATCTGCGCTTATGCCAATGCCGGCTTTACCGCCGAGCGTGCCGGGCAGCTAAGCCGCCTGACGGGAACTCACGTTCCGGCAGGAACTGGCACAGAAGCGGCATCTTTGCGCGACAGTCTCTGCCTGCTGCAAAAAAGCTATCGTTTCGGCAGCGATTCTGGCATTGGTCAGTTAGCTGCGGCGATCAACCGTGGTGATAAAACGGCAGTGAAAATCGTTTTTCAGCAGG\n",
      "\n",
      "GTCGAAAGACCAGGGCATTGTCGCCTTCATTATGGATGTCATCCCGGCGAGCGTCTTTGGCGCATTTGCCAGCGGTAACATTCTGCAGGTGCTGCTGTTTGCCGTACTGTTTGGTTTTGCGCTCCACCGTCGTGGCAGCTAT\n",
      "\n",
      "GGTGCTGTTTGTGGCAACCCACTGGTGAGTTTTTTCCAGTCAACATTGTCTTCGGTGAAAATCTTGCCATCAAGAACGCGAACCACCAGATCGGAGATAGCCAGGAAGCTGCTCGGTTGGTCGATGACAATCGGTGCCCCCTGATGTGGTGCCTTCATGCCGAAGAATTTCACCCCAACGGGGACGTCGGTGATAGACGGGCTAGGGATATCACGTAGGCCAGATACCTGCATTCT\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[1;32m---> 17\u001b[0m     line1,line2,line3,line4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     19\u001b[0m     name, illumin, length \u001b[38;5;241m=\u001b[39m line1\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mjonah\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mSchool\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mASI\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mbloom\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43msample.fastq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36mopen_file\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mjonah\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSchool\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mASI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbloom\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msample.fastq\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m sample:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[1;32m---> 17\u001b[0m         line1,line2,line3,line4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m         name, illumin, length \u001b[38;5;241m=\u001b[39m line1\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     20\u001b[0m         seq\u001b[38;5;241m=\u001b[39mline2\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "open_file(\"C:\\\\Users\\\\jonah\\\\School\\\\ASI\\\\bloom\\\\sample.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2bdc637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T16:52:42.297817Z",
     "start_time": "2022-07-23T16:52:42.280815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@SRR19897826.4.2 4 length=251\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open(\"C:\\\\Users\\\\jonah\\\\School\\\\ASI\\\\bloom\\\\sample.fastq\")\n",
    "\n",
    "next(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ede2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
